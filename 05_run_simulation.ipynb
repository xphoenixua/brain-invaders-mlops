{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d05964-7000-4970-9e3e-c516327fd8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import mlflow\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "dbutils.widgets.text(\"processed_data_path\", \"/dbfs/FileStore/tables/p300_files/processed-features/\", \"DBFS path to processed features\")\n",
    "dbutils.widgets.text(\"serving_endpoint_name\", \"p300\", \"Name of the Serving Endpoint\")\n",
    "dbutils.widgets.text(\"target_entity_name\", \"champion\", \"Served entity to test (like 'champion' or 'challenger')\")\n",
    "dbutils.widgets.text(\"num_subjects_to_test\", \"5\", \"Number of subjects to sample for testing\")\n",
    "dbutils.widgets.text(\"registered_model_name\", \"P300_Classifier\", \"Registered Model Name\")\n",
    "dbutils.widgets.text(\"experiment_name\", f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/P300_BCI_Inference\", \"MLflow Experiment Name\")\n",
    "\n",
    "# get widget values\n",
    "processed_data_root_path = dbutils.widgets.get(\"processed_data_path\")\n",
    "serving_endpoint_name = dbutils.widgets.get(\"serving_endpoint_name\")\n",
    "target_entity_name = dbutils.widgets.get(\"target_entity_name\")\n",
    "num_subjects_to_test = int(dbutils.widgets.get(\"num_subjects_to_test\"))\n",
    "registered_model_name = dbutils.widgets.get(\"registered_model_name\")\n",
    "mlflow_experiment_name = dbutils.widgets.get(\"experiment_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25c819a-d3e3-490d-8593-44995f1a975a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- configure API clients and endpoint URL ---\n",
    "databricks_host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "databricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "headers = {\"Authorization\": f\"Bearer {databricks_token}\", \"Content-Type\": \"application/json\"}\n",
    "invocation_url = f\"{databricks_host}/serving-endpoints/{serving_endpoint_name}/served-models/{target_entity_name}/invocations\"\n",
    "print(f\"will send inference requests to: {invocation_url}\")\n",
    "\n",
    "# configure MLflow client to handle both legacy and UC model names\n",
    "mlflow.set_registry_uri(\"databricks-uc\" if \".\" in registered_model_name else \"databricks\")\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "mlflow_client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865451f9-388f-4bca-bd8d-f151278d00a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- data selection logic with holdout set creation ---\n",
    "\n",
    "# 1. query the serving endpoint API to find which model version the target entity is serving\n",
    "print(f\"querying endpoint '{serving_endpoint_name}' to find version for entity '{target_entity_name}'...\")\n",
    "endpoint_url_get = f\"{databricks_host}/api/2.0/serving-endpoints/{serving_endpoint_name}\"\n",
    "response = requests.get(endpoint_url_get, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "endpoint_config = response.json()\n",
    "# look inside the 'config' and its 'served_entities' list\n",
    "served_entities = endpoint_config.get(\"config\", {}).get(\"served_entities\", [])\n",
    "served_model_version = None\n",
    "served_model_name = None\n",
    "\n",
    "for entity in served_entities:\n",
    "    if entity.get(\"name\") == target_entity_name:\n",
    "        served_model_version = entity.get(\"entity_version\")\n",
    "        served_model_name = entity.get(\"entity_name\")\n",
    "        break\n",
    "\n",
    "print(f\"found that entity '{target_entity_name}' is serving model '{served_model_name}' version '{served_model_version}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878093f1-8396-4a47-888c-99f94b0c41f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. get the run ID associated with that specific model version\n",
    "mlflow.set_registry_uri(\"databricks\") # set to legacy workspace registry\n",
    "mlflow_client = mlflow.tracking.MlflowClient()\n",
    "model_version_details = mlflow_client.get_model_version(name=served_model_name, version=served_model_version)\n",
    "served_model_run_id = model_version_details.run_id\n",
    "\n",
    "# 3. download the training_subjects.json artifact from that run\n",
    "display(served_model_run_id)\n",
    "local_path = mlflow_client.download_artifacts(run_id=served_model_run_id, path=\"training_subjects.json\")\n",
    "with open(local_path, 'r') as f:\n",
    "    training_subjects_data = json.load(f)\n",
    "training_subjects_set = set(training_subjects_data.get(\"training_subjects\", []))\n",
    "print(f\"model version {served_model_version} was trained on {len(training_subjects_set)} subjects. they will be excluded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a559372a-6440-4380-a7f8-1f4621eb7c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. create the holdout set and sample from it\n",
    "all_available_subjects_set = {d for d in os.listdir(processed_data_root_path)}\n",
    "testing_holdout_set = all_available_subjects_set - training_subjects_set\n",
    "holdout_list = sorted(list(testing_holdout_set))\n",
    "\n",
    "print(f\"found {len(holdout_list)} subjects available for testing.\")\n",
    "\n",
    "subjects_to_test = holdout_list\n",
    "if len(holdout_list) >= num_subjects_to_test:\n",
    "    subjects_to_test = random.sample(holdout_list, k=num_subjects_to_test)\n",
    "else:\n",
    "    print(f\"warning: requested {num_subjects_to_test}, but only {len(holdout_list)} available in holdout set. using all available.\")\n",
    "\n",
    "print(f\"final subjects selected for testing: {subjects_to_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61572594-8f8a-4273-885d-78fa93af8049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- inference and evaluation loop ---\n",
    "all_true_labels, all_pred_labels, all_pred_probas = [], [], []\n",
    "INFERENCE_BATCH_SIZE = 100 \n",
    "\n",
    "for subject_name in subjects_to_test:\n",
    "    print(f\"\\nprocessing subject {subject_name}...\")\n",
    "    subject_folder_path = os.path.join(processed_data_root_path, subject_name)\n",
    "    \n",
    "    features_df = pd.read_parquet(os.path.join(subject_folder_path, \"features.parquet\"))\n",
    "    labels_df = pd.read_parquet(os.path.join(subject_folder_path, \"labels.parquet\"))\n",
    "    \n",
    "    num_features = features_df.shape[1]\n",
    "    features_df.columns = [f\"feature_{i}\" for i in range(num_features)]\n",
    "\n",
    "    # --- Mini-batching loop ---\n",
    "    subject_pred_list = []\n",
    "    num_rows = len(features_df)\n",
    "    \n",
    "    print(f\"  sending {num_rows} records in batches of {INFERENCE_BATCH_SIZE}...\")\n",
    "    for i in range(0, num_rows, INFERENCE_BATCH_SIZE):\n",
    "        batch_df = features_df.iloc[i : i + INFERENCE_BATCH_SIZE]\n",
    "        \n",
    "        data_to_send = {\n",
    "            \"dataframe_split\": {\n",
    "                \"columns\": batch_df.columns.tolist(),\n",
    "                \"data\": batch_df.values.tolist()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.post(invocation_url, headers=headers, json=data_to_send)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        response_data = response.json()\n",
    "        predictions_batch = response_data.get(\"predictions\", [])\n",
    "        subject_pred_list.extend(predictions_batch)\n",
    "    # --- End of mini-batching loop ---\n",
    "    \n",
    "    subject_pred_labels = [p.get('predicted_label_code') for p in subject_pred_list]\n",
    "    subject_pred_probas = [p.get('probability_target') for p in subject_pred_list]\n",
    "    subject_true_labels = labels_df['label'].values\n",
    "    \n",
    "    all_true_labels.extend(subject_true_labels)\n",
    "    all_pred_labels.extend(subject_pred_labels)\n",
    "    all_pred_probas.extend(subject_pred_probas)\n",
    "    print(f\"  successfully received {len(subject_pred_list)} predictions for {subject_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217b9afd-cbc7-4c6b-b52c-ba9be62129fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- final metrics calculation and logging ---\n",
    "if not all_true_labels:\n",
    "    print(\"\\nno predictions were made. cannot calculate final metrics.\")\n",
    "else:\n",
    "    TARGET_CLASS_LABEL_CODE = 2\n",
    "    \n",
    "    true_binary = np.where(np.array(all_true_labels) == TARGET_CLASS_LABEL_CODE, 1, 0)\n",
    "    pred_binary = np.where(np.array(all_pred_labels) == TARGET_CLASS_LABEL_CODE, 1, 0)\n",
    "    \n",
    "    accuracy = accuracy_score(true_binary, pred_binary)\n",
    "    kappa = cohen_kappa_score(true_binary, pred_binary)\n",
    "    auc = None\n",
    "    if all(p is not None for p in all_pred_probas) and len(np.unique(true_binary)) > 1:\n",
    "        auc = roc_auc_score(true_binary, all_pred_probas)\n",
    "    \n",
    "    # log results to a new mlflow run\n",
    "    with mlflow.start_run(run_name=f\"evaluation_{target_entity_name}_v{served_model_version}\") as run:\n",
    "        mlflow.log_param(\"evaluated_model_name\", registered_model_name)\n",
    "        mlflow.log_param(\"evaluated_model_version\", served_model_version)\n",
    "        mlflow.log_param(\"evaluated_entity_alias\", target_entity_name)\n",
    "        mlflow.log_param(\"num_test_subjects\", len(subjects_to_test))\n",
    "        \n",
    "        metrics_to_log = {\"evaluation_accuracy\": accuracy, \"evaluation_kappa\": kappa}\n",
    "        if auc: metrics_to_log[\"evaluation_auc\"] = auc\n",
    "        mlflow.log_metrics(metrics_to_log)\n",
    "        \n",
    "        print(f\"logged evaluation metrics to MLflow run: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_run_simulation",
   "widgets": {
    "experiment_name": {
     "currentValue": "/Users/kalapun.pn@ucu.edu.ua/P300_BCI_Inference",
     "nuid": "31e37e4e-f315-4f9c-abfc-78f21f20ed8d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Users/kalapun.pn@ucu.edu.ua/P300_BCI_Inference",
      "label": "MLflow Experiment Name",
      "name": "experiment_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Users/kalapun.pn@ucu.edu.ua/P300_BCI_Inference",
      "label": "MLflow Experiment Name",
      "name": "experiment_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "num_subjects_to_test": {
     "currentValue": "5",
     "nuid": "4b36d603-962b-4ea9-a2c2-1daa806456e4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": "Number of subjects to sample for testing",
      "name": "num_subjects_to_test",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "Number of subjects to sample for testing",
      "name": "num_subjects_to_test",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "processed_data_path": {
     "currentValue": "/dbfs/FileStore/tables/p300_files/processed-features/",
     "nuid": "152fa556-a23c-412c-be2f-6f12770921cf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/dbfs/FileStore/tables/p300_files/processed-features/",
      "label": "DBFS path to processed features",
      "name": "processed_data_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/dbfs/FileStore/tables/p300_files/processed-features/",
      "label": "DBFS path to processed features",
      "name": "processed_data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "registered_model_name": {
     "currentValue": "P300_Classifier",
     "nuid": "cf39f29a-e9f7-4c01-9ab1-166d3df6c614",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "P300_Classifier",
      "label": "Registered Model Name",
      "name": "registered_model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "P300_Classifier",
      "label": "Registered Model Name",
      "name": "registered_model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "serving_endpoint_name": {
     "currentValue": "p300",
     "nuid": "40a622b2-dd36-4c82-9d2a-2623184c9834",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "p300",
      "label": "Name of the Serving Endpoint",
      "name": "serving_endpoint_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "p300",
      "label": "Name of the Serving Endpoint",
      "name": "serving_endpoint_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_entity_name": {
     "currentValue": "challenger",
     "nuid": "a3a6527e-62cb-4573-bc98-dcea6806f391",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "champion",
      "label": "Served entity to test (like 'champion' or 'challenger')",
      "name": "target_entity_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "champion",
      "label": "Served entity to test (like 'champion' or 'challenger')",
      "name": "target_entity_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
